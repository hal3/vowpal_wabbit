Generating 3-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = models/0001.model
experience replay level=b, buffer=100, replay count=1
Num weight bits = 18
learning rate = 2.56e+06
initial_t = 128000
power_t = 1
decay_learning_rate = 1
creating cache_file = train-sets/0001.dat.cache
Reading datafile = train-sets/0001.dat
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0   1.0000   0.0000      290
0.500037 0.000074            2            2.0   0.0000   0.0086      608
0.250094 0.000151            4            4.0   0.0000   0.0040      794
0.248153 0.246212            8            8.0   0.0000   0.0242      860
0.302406 0.356658           16           16.0   1.0000   0.0460      128
0.317257 0.332108           32           32.0   0.0000   0.0597      176
0.315004 0.312752           64           64.0   0.0000   0.1341      350
0.307500 0.299996          128          128.0   1.0000   0.2871      620
0.230417 0.153334          256          256.0   0.0000   0.1287      410
0.115480 0.000543          512          512.0   0.0000   0.0043      278
0.057740 0.000000         1024         1024.0   1.0000   1.0000      170

finished run
number of examples per pass = 200
passes used = 8
weighted example sum = 1600.000000
weighted label sum = 728.000000
average loss = 0.036954
best constant = 0.455000
best constant's loss = 0.247975
total feature number = 717536
